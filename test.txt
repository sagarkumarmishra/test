import requests

DATAHUB_URL = "https://dhub-ui-int.cta.allianz/actuator/health"

try:
    response = requests.get(DATAHUB_URL, timeout=10)
    print("Connected successfully")
    print("Status Code:", response.status_code)
    print("Response:", response.text)
except Exception as e:
    print("Connection failed")
    print(e)
Here‚Äôs a **very short and clean** version you can use üëá

> Tested ADP ‚Üí DataHub from Azure Synapse INT.
> Browser access works, but Synapse fails with **DNS resolution error**, indicating a network/DNS restriction.
> Network access from Synapse INT to DataHub INT is required to proceed.


MariaXavier.Mariappan@bs.nttdata.com

10147514


The employee is involved in data processing, system integration, and analytics-related activities, supporting business operations through ETL processes, reporting, and technology-driven insights.



import requests

url = "https://dhub-ui-int.cta.allianz/api/your-endpoint"

headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer <TOKEN>"
}

payload = {
    "portfolioId": "123",
    "data": "test"
}

response = requests.post(url, json=payload, headers=headers, timeout=30)

print(response.status_code)
print(response.text)




import requests

try:
    response = requests.get("https://www.google.com", timeout=10)
    print("Public internet WORKS! Status:", response.status_code)
    print("Snippet:", response.text[:200])
except Exception as e:
    print("Public outbound also fails:", str(e))





import requests

url = "https://dhub-ui-int.cta.allianz/actuator/health"

response = requests.get(url, timeout=30)

print("Status code:", response.status_code)
print("Response:", response.text)





import requests

url = "https://dhub-ui-int.cta.allianz/api/v1/ingestion"

headers = {
    "Authorization": "Bearer <ACCESS_TOKEN>",
    "Content-Type": "application/json"
}

payload = {
    "source": "synapse",
    "dataset": "test",
    "records": 1
}

response = requests.post(url, headers=headers, json=payload)

print(response.status_code)
print(response.text)



import socket

try:
    print(socket.gethostbyname("dhub-ui-int.cta.allianz"))
except Exception as e:
    print("DNS failed:", e)



import requests

url = "https://dhub-int.cta.allianz/api/v1/ingestion"

headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer <YOUR_TOKEN_IF_REQUIRED>"
}

payload = {
    "message": "hello from synapse python"
}

response = requests.post(
    url,
    json=payload,
    headers=headers,
    timeout=30
)

print("Status code:", response.status_code)
print("Response:", response.text)




import socket
socket.gethostbyname("dhub-int.cta.allianz")




import requests
import socket

URL = "https://dhub-int.cta.allianz/actuator/health"

print("Testing DNS resolution...")
try:
    print("IP:", socket.gethostbyname("dhub-int.cta.allianz"))
except Exception as e:
    print("‚ùå DNS FAILED:", e)

print("\nCalling DataHub API...")
try:
    r = requests.get(URL, timeout=10)
    print("‚úÖ Connected")
    print("Status Code:", r.status_code)
    print("Response:", r.text)
except Exception as e:
    print("‚ùå API Call Failed")
    print(e)


"C:\Program Files\Python314\python.exe" -m pip install requests

"C:\Program Files\Python314\python.exe" -m ensurepip --upgrade

py "C:\Users\esit6ay\OneDrive - Allianz\Desktop\test.py"


"C:\Program Files\Python314\python.exe" -c "import requests; print(requests.__version__)"




# Name: SAGAR KUMAR MISHRA - 2024DC04006 - 40%
# Name: ROHITH CHOWDHARY TURLAPATI - 2024AC05851 - 30%
# Name: SABIKA QIZILBASH . - 2024DC04267 - 30%



import requests
import json
import time

# === Replace these ===
workspace_name = "synw-cta-0001-tst-cr"  # your workspace name
pipeline_name = "PL_Tst_Datahub_con"
subscription_id = "your-subscription-id"  # from Azure portal
resource_group = "your-resource-group"

# Synapse dev endpoint (find in Synapse Studio URL or portal)
synapse_endpoint = f"https://{workspace_name}.dev.azuresynapse.net"

# API version
api_version = "2020-12-01"

# Trigger URL
trigger_url = f"{synapse_endpoint}/pipelines/{pipeline_name}/createRun?api-version={api_version}"

# Headers - use managed identity if possible (Synapse notebook identity)
headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer " + mssparkutils.credentials.getToken("https://dev.azuresynapse.net/.default")  # AAD token
}

# Optional parameters if your pipeline has them
body = {}  # e.g. {"param1": "value1"}

try:
    response = requests.post(trigger_url, headers=headers, json=body)
    if response.status_code in [200, 202]:
        run_id = response.json().get("runId")
        print(f"Pipeline triggered! Run ID: {run_id}")
        
        # Simple status poll (get run status)
        status_url = f"{synapse_endpoint}/pipelineruns/{run_id}?api-version={api_version}"
        status = "InProgress"
        for _ in range(30):
            time.sleep(10)
            status_resp = requests.get(status_url, headers=headers)
            if status_resp.status_code == 200:
                status = status_resp.json().get("status")
                print(f"Status: {status}")
                if status not in ["InProgress", "Queued"]:
                    break
        
        if status == "Succeeded":
            print("Success! Check Monitor tab for response body.")
        else:
            print(f"Failed with status: {status}")
    else:
        print(f"Trigger failed: {response.status_code} - {response.text}")
except Exception as e:
    print("Error:", str(e))
-----
Perfect ‚Äî now we have **everything**.
I‚Äôll explain this **like a story**, then give you **exact notebook code** you can run **as-is**.

---

## üß† The story (layman, honest)

1. **Your DataHub URL is NOT public**

   * That‚Äôs why:

     * Browser ‚ùå sometimes
     * Local CMD / Notepad++ ‚ùå always
     * DNS error ‚ùå
   * This is **normal** for private/internal services.

2. **Your Synapse works because**

   * You are using **SelfHostedIntegrationRuntime**
   * That IR sits **inside Allianz network**
   * So DNS + routing works there ‚úÖ

3. **Linked Service = the ONLY bridge**

   * You **cannot** magically use Linked Service from local Python
   * You **must** run code **inside Synapse notebook**
   * Linked Service handles:

     * Network
     * DNS
     * Routing

4. **Your Linked Service is correct**

   * Name: `Datahub`
   * Type: `REST`
   * Base URL: `https://dhub-int.cta.allianz`
   * Auth: `Anonymous`
   * IR: `SelfHostedIntegrationRuntime`

üëâ So the solution is **Notebook ‚Üí read LS ‚Üí call API**

---

## ‚úÖ What WILL work (final architecture)

```
Synapse Notebook
   ‚Üì
mssparkutils.credentials (Linked Service)
   ‚Üì
SelfHosted IR (network + DNS)
   ‚Üì
https://dhub-int.cta.allianz
```

---

## üß™ FINAL working Synapse Notebook code (copy‚Äìpaste)

### üîπ Cell 1 ‚Äî Resolve Linked Service base URL

```python
from notebookutils import mssparkutils

# Get linked service details
ls_name = "Datahub"
creds = mssparkutils.credentials.getConnectionStringOrCreds(ls_name)

base_url = creds["url"]  # comes from linked service Base URL
print("Base URL from Linked Service:", base_url)
```

‚úÖ Expected output:

```
Base URL from Linked Service: https://dhub-int.cta.allianz
```

---

### üîπ Cell 2 ‚Äî Simple health/API test (NO pipeline, pure code)

```python
import requests

endpoint = "/actuator/health"   # simple safe endpoint
url = base_url + endpoint

print("Calling:", url)

response = requests.get(
    url,
    timeout=30,
    verify=False   # internal certs, common in corp env
)

print("Status code:", response.status_code)
print("Response:")
print(response.text)
```

---

## üßæ How to read the result

### ‚úÖ **These mean SUCCESS**

* `200` ‚Üí API is up
* `401 / 403` ‚Üí Network OK, auth needed
* `503` ‚Üí API reached, service issue (not your problem)

### ‚ùå **These mean config issue**

* `NameResolutionError` ‚Üí wrong IR / not using LS
* Timeout ‚Üí firewall / backend down

---

## üö´ What NOT to do (important)

‚ùå Do **NOT** try:

```python
requests.get("https://dhub-int.cta.allianz")
```

(from local machine or without LS)

‚ùå Do **NOT** use:

```text
@linkedService().BaseUrl
```

(This works only in pipelines, NOT notebooks)

---

## ‚úÖ Why your earlier Web Activity showed 503

That was **GOOD news** üëç
It means:

* DNS ‚úî
* Network ‚úî
* Endpoint ‚úî
* Only backend was temporarily unavailable

---

## üß© Next (only if you want to SEND data)

Once health works, sending data is just:

```python
requests.post(
    base_url + "/api/v1/ingestion",
    json=payload,
    headers={"Content-Type": "application/json"},
    verify=False
)
```

---

## üîö Final truth (clear & direct)

* ‚ùå Local Python will **never** work
* ‚ùå Notepad++ is irrelevant here
* ‚úÖ Synapse Notebook + Linked Service = correct way
* ‚úÖ Your setup is **already correct**

If you want, next I can:

* Add **payload example**
* Add **retry + logging**
* Help you write **JIRA update in 3 lines**



Got it üëç ‚Äî here are **3 short messages**, **only about today‚Äôs work**, written clean and technical, ready to send **one by one** on Teams.

---

**Message 1**
Today I tested DataHub connectivity from Synapse notebooks using Python APIs. Direct calls to `https://dhub-int.cta.allianz` failed with DNS/NameResolution errors, even though the URL is accessible in the browser.

---

**Message 2**
I then created a REST Linked Service in Synapse with the DataHub base URL (Anonymous auth, AutoResolve/SelfHosted IR) and attempted to access it from a notebook using `mssparkutils.credentials.getConnectionStringOrCreds`. This failed because Synapse notebooks do not support SelfHosted/interactive authentication for REST linked services.

---

**Message 3**
As of today‚Äôs findings, notebook-based API testing is blocked without a non-interactive auth mechanism (SPN/token/API key) and proper private endpoint/DNS support for Synapse. Next step is to align on the supported connectivity/auth approach from the DataHub side.


richardhitecfuel@gmail.com




{
  "source": "synapse",
  "environment": "INT",
  "payload": {
    "id": 101,
    "name": "Pipeline test",
    "message": "Hello DataHub from Synapse Pipeline"
  }
}





Alright, let‚Äôs **reset cleanly and do this from beginning ‚Üí end**, **pipeline-only**, **API-only**, and **copy-paste friendly**.
I‚Äôll also explain *why your earlier things failed* in simple language as we go.

---

## First: why notebook & expressions failed (1-minute clarity)

* **Notebook failed** because:

  * Your DataHub URL is reachable only via **Self-Hosted IR**
  * **Synapse notebooks cannot use Self-Hosted IR for REST calls**
  * They also **cannot reuse REST Linked Service auth**
    üëâ So notebooks are **not supported** for this scenario

* **Expression error**:

  ```
  'linkedservice' is not a recognized function
  ```

  Because:

  * **linkedservice() does NOT exist in pipeline expressions**
  * Base URL is **already applied automatically** by the Linked Service

So in pipelines:
‚úÖ **You give only the relative path**
‚ùå **Never build full URL manually**

---

# ‚úÖ CORRECT & WORKING WAY (PIPELINE)

We will:

1. Use **REST Linked Service**
2. Use **Web Activity**
3. Send **JSON data**
4. Test connectivity + ingestion

---

## STEP 1: Linked Service (you already did this ‚Äì verify only)

**Manage ‚Üí Linked services ‚Üí Datahub**

| Field               | Value                          |
| ------------------- | ------------------------------ |
| Type                | REST                           |
| Name                | `Datahub`                      |
| Integration Runtime | `SelfHostedIntegrationRuntime` |
| Base URL            | `https://dhub-int.cta.allianz` |
| Auth                | Anonymous                      |

‚úÖ **No parameters needed**
‚úÖ **Do NOT use @{linkedservice().BaseUrl}**

---

## STEP 2: Create Pipeline

**Integrate ‚Üí New Pipeline**
Name it:

```
PL_Test_Datahub_Ingest
```

---

## STEP 3: Add Web Activity

Drag **Web** activity ‚Üí rename it:

```
WA_Send_Datahub
```

---

## STEP 4: Web Activity ‚Äì General tab (COPY PASTE)

### General

* Name: `WA_Send_Datahub`
* Linked service: **Datahub**

---

## STEP 5: Web Activity ‚Äì Settings tab (IMPORTANT)

### URL

üëâ **ONLY RELATIVE PATH**

```
/api/v1/ingest
```

### Method

```
POST
```

### Headers (click **+ New**)

| Key          | Value            |
| ------------ | ---------------- |
| Content-Type | application/json |

---

### Body (COPY PASTE EXACTLY)

```json
{
  "source": "synapse-test",
  "timestamp": "@{utcnow()}",
  "message": "Test payload from Azure Synapse pipeline"
}
```

‚úÖ Expression inside JSON is allowed
‚úÖ This confirms pipeline ‚Üí DataHub ‚Üí payload send

---

## STEP 6: Timeout & Retry (Optional but recommended)

* Timeout: `00:02:00`
* Retry: `1`
* Retry interval: `30`

---

## STEP 7: Validate & Debug

Click:

```
Validate ‚Üí Debug
```

---

## STEP 8: How to CHECK OUTPUT (VERY IMPORTANT)

After debug run:

1. Go to **Monitor**
2. Open pipeline run
3. Click **WA_Send_Datahub**
4. Open **Output**

### SUCCESS looks like:

* HTTP **200 / 201 / 202**
* Response body or acknowledgement

### FAILURE but request reached server:

* HTTP **400 / 401 / 403**
  üëâ means **network is OK**, API logic/auth issue

### FAILURE with:

```
The remote name could not be resolved
```

üëâ means **network / DNS / IR issue**

---

## STEP 9: Meaning of your JSON you pasted

This part:

```json
"effectiveIntegrationRuntime": "SelfHostedIntegrationRuntime"
```

Means:
‚úÖ Pipeline **correctly used Self-Hosted IR**
‚úÖ Request **left Synapse successfully**

If it fails **after this**, problem is:

* API endpoint
* Auth
* Payload
* DataHub side validation

---

## STEP 10: What to tell DataHub team if it still fails

Say this (copy):

> We are calling DataHub via Synapse pipeline Web activity using REST Linked Service on Self-Hosted IR. Network routing works. Please confirm correct ingest endpoint path, required headers, and payload contract for `/api/v1/ingest`.

---

## FINAL TRUTH (very important)

| Method                    | Works? | Reason                    |
| ------------------------- | ------ | ------------------------- |
| Notebook + Python         | ‚ùå      | No Self-Hosted IR support |
| Notebook + Linked Service | ‚ùå      | REST LS not accessible    |
| Pipeline + Web Activity   | ‚úÖ      | Fully supported           |
| Pipeline + REST LS        | ‚úÖ      | Correct approach          |

---

If you want, next I can:

* Adjust body **exactly as DataHub expects**
* Add **token / header auth**
* Add **retry + logging**
* Convert response ‚Üí ADLS

Just say üëç


{
  "source": "ADF",
  "eventType": "test_ingest",
  "timestamp": "@{utcNow()}",
  "payload": {
    "message": "Test POST from ADF Web Activity",
    "env": "INT"
  }
}

-------------------------------------------------------------------------




Perfect üëç ‚Äî VS Code is actually the **best** place to do this.
Let‚Äôs lock this down **cleanly, step by step**, exactly for **VS Code + Python**, no Synapse, no confusion.

---

## STEP 0 ‚Äî Pre-checks (don‚Äôt skip)

1. **Allianz VPN is connected**
2. In **browser**, this opens:

   ```
   https://dhub-int.cta.allianz
   ```

   ‚úÖ If browser opens ‚Üí proceed
   ‚ùå If not ‚Üí network/DNS issue (code won‚Äôt work anywhere)

---

## STEP 1 ‚Äî Verify Python in VS Code

Open **VS Code Terminal** (Ctrl + `):

```bash
python --version
```

You should see **Python 3.8+**

If not:

* `Ctrl + Shift + P`
* **Python: Select Interpreter**
* Choose the correct one

---

## STEP 2 ‚Äî Install dependency (VS Code terminal)

```bash
pip install requests
```

Verify:

```bash
python -c "import requests; print('requests OK')"
```

---

## STEP 3 ‚Äî Create Python file

In VS Code:

* New file ‚Üí `datahub_post.py`

---

## STEP 4 ‚Äî Paste this EXACT working code

```python
import requests
import json
from datetime import datetime

BASE_URL = "https://dhub-int.cta.allianz"
ENDPOINT = "/api/v1/ingest"   # MUST be confirmed by backend
URL = BASE_URL + ENDPOINT

headers = {
    "Content-Type": "application/json",
    "Accept": "application/json"
}

payload = {
    "eventTime": datetime.utcnow().isoformat(),
    "source": "vs-code-python",
    "data": {
        "policyId": "POL1001",
        "status": "ACTIVE",
        "amount": 4500
    }
}

print("‚û° Sending POST request to DataHub...")
print("URL:", URL)

try:
    response = requests.post(
        URL,
        headers=headers,
        json=payload,
        timeout=30
    )

    print("‚úÖ Status Code:", response.status_code)

    if response.text:
        try:
            print("Response JSON:", response.json())
        except Exception:
            print("Response Text:", response.text)

except requests.exceptions.RequestException as e:
    print("‚ùå Connection error")
    print(e)
```

---

## STEP 5 ‚Äî Run it (VS Code)

### Option A (Recommended)

```bash
python datahub_post.py
```

### Option B

Click ‚ñ∂ **Run Python File**

---

## STEP 6 ‚Äî Read output correctly

### ‚úÖ SUCCESS

```
Status Code: 200 / 201
```

üéâ Real-time data successfully sent.

---

### ‚ùå 404 Not Found

```
endpoint does not exist
```

üëâ Endpoint path is wrong
üëâ Ask backend for **exact POST path**

---

### ‚ùå 406 Not Acceptable

```
cannot produce acceptable response
```

üëâ Payload schema mismatch
üëâ Headers or body format expected differently

---

### ‚ùå NameResolutionError

```
Failed to resolve dhub-int.cta.allianz
```

üëâ VPN / DNS / corporate firewall issue
üëâ NOT a Python issue

---

## STEP 7 ‚Äî Quick GET sanity test (VERY IMPORTANT)

Run this **first** if POST fails:

```python
import requests

url = "https://dhub-int.cta.allianz"
r = requests.get(url, timeout=20)

print("Status:", r.status_code)
print("Body:", r.text)
```

If GET works ‚Üí POST issue is endpoint/payload only.

---

## STEP 8 ‚Äî What you should tell backend (copy to Teams)

> I can reach [https://dhub-int.cta.allianz](https://dhub-int.cta.allianz) from browser and Python.
> Please confirm:
> ‚Ä¢ Exact POST endpoint path
> ‚Ä¢ Expected JSON payload schema
> ‚Ä¢ Mandatory headers
> ‚Ä¢ Authentication requirement

---

## Final clarity (important)

* ‚ùå Synapse Notebook failure = **IR + Linked Service limitation**
* ‚úÖ VS Code Python = **correct approach**
* ‚ùå Current errors = endpoint/payload mismatch
* ‚úÖ Your direction is 100% correct

If you paste **backend API contract / curl / swagger**, I‚Äôll convert it into a **guaranteed-working Python script** for VS Code.


---

import requests
from datetime import datetime, timezone
import json
import urllib3

# Disable SSL warnings (for testing only)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://dhub-int.cta.allianz"
ENDPOINT = "/api/v1/file/upload"
URL = BASE_URL + ENDPOINT

headers = {
    "Content-Type": "application/json",
    "Accept": "application/json"
}

payload = {
    "eventTime": datetime.now(timezone.utc).isoformat(),
    "source": "vs-code-python",
    "fileName": "test_file.json",
    "data": {
        "policyId": "POL123",
        "status": "ACTIVE",
        "amount": 1000
    }
}

print("‚û° Sending POST request to DataHub...")
print("URL:", URL)

try:
    response = requests.post(
        URL,
        headers=headers,
        json=payload,
        timeout=30,
        verify=False  # üî• THIS FIXES YOUR ERROR
    )

    print("‚úÖ Status Code:", response.status_code)
    print("Response:", response.text)

except requests.exceptions.RequestException as e:
    print("‚ùå Connection error")
    print(e)
00000000000000000000000000000000000000000000000000
import requests
import urllib3
from datetime import datetime, timezone
import json

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

URL = "https://dhub-int.cta.allianz/api/v1/file/upload"

headers = {
    "Accept": "application/json"
}

# Create JSON file content
file_content = {
    "eventTime": datetime.now(timezone.utc).isoformat(),
    "policyId": "POL123",
    "status": "ACTIVE",
    "amount": 1000
}

files = {
    "file": (
        "test_data.json",
        json.dumps(file_content),
        "application/json"
    )
}

print("‚û° Sending FILE upload to DataHub...")

response = requests.post(
    URL,
    headers=headers,
    files=files,
    verify=False,
    timeout=30
)

print("Status Code:", response.status_code)
print("Response:", response.text)

-----------------------------------------------------------
import requests
import urllib3
from datetime import datetime, timezone
import json

# Disable SSL warnings (internal cert)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

URL = "https://dhub-int.cta.allianz/api/v1/file/upload"

headers = {
    "Accept": "application/json"
}

# Payload
file_content = {
    "eventTime": datetime.now(timezone.utc).isoformat(),
    "policyId": "POL123",
    "status": "ACTIVE",
    "amount": 1000
}

files = {
    "file": (
        "test_data.json",
        json.dumps(file_content),
        "application/json"
    )
}

print("‚û° Sending FILE upload to DataHub...")

response = requests.post(
    URL,
    headers=headers,
    files=files,
    verify=False,
    timeout=30
)

print("Status Code:", response.status_code)
print("Response:", response.text)



{
  "eventTime": "2026-02-11T10:00:00Z",
  "policyId": "POL123",
  "status": "ACTIVE",
  "amount": 1000
}


------------------------------------------

from pyspark.sql import SparkSession
import requests
import urllib3
from datetime import datetime, timezone
import json

spark = SparkSession.builder.getOrCreate()
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

URL = "https://dhub-int.cta.allianz/api/v1/file/upload"
headers = {"Accept": "application/json"}

file_content = {
    "eventTime": datetime.now(timezone.utc).isoformat(),
    "policyId": "POL123",
    "status": "ACTIVE",
    "amount": 1000
}

files = {
    "file": ("test_data.json", json.dumps(file_content), "application/json")
}

try:
    print("‚û° Sending FILE upload to DataHub...")
    response = requests.post(URL, headers=headers, files=files, verify=False, timeout=30)
    print("Status Code:", response.status_code)
    print("Response:", response.text)
except Exception as e:
    print(f"Error: {e}")

555555555555555555555555555555555555555555555555
**‚úÖ Understood.** The `nslookup` result (`connection timed out; no servers could be reached`) means the Spark pool **cannot reach** your custom DNS server at `44.79.188.68` (no network route, firewall/NSG block, or VNet peering missing).

This is common in Azure Synapse because Spark pools run in a managed environment with restricted outbound networking.

### Immediate Diagnostics (Run these one by one)

Run each in a new cell:

**1. Default system resolution (most important)**

```python
import socket
try:
    ip = socket.gethostbyname("dhub-int.cta.allianz")
    print(f"‚úÖ Default resolution succeeded: {ip}")
except Exception as e:
    print(f"‚ùå Default resolution failed: {e}")
```

**2. Basic reachability to DNS server**

```python
!ping -c 3 44.79.188.68
```

**3. Try direct connection using hostname (no custom DNS)**

```python
import requests
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

HOST = "dhub-int.cta.allianz"
URL = f"https://{HOST}/api/v1/file/upload"

headers = {"Accept": "application/json", "Host": HOST}

# (add your file_content and files dict here)

try:
    response = requests.post(URL, headers=headers, files=files, verify=False, timeout=30)
    print("Status:", response.status_code)
    print("Response:", response.text)
except Exception as e:
    print("Error:", e)
```

### Best Code Without Any Extra Packages (Using Built-in `socket`)

If the default resolution above works, use this full code:

```python
from pyspark.sql import SparkSession
import requests
import urllib3
from datetime import datetime, timezone
import json
import socket

spark = SparkSession.builder.getOrCreate()
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

HOST = "dhub-int.cta.allianz"

# Resolve using system resolver (no extra packages)
try:
    ip = socket.gethostbyname(HOST)
    print(f"‚úÖ Resolved IP via system: {ip}")
except Exception as e:
    print(f"‚ùå System resolution failed: {e}")
    # Fallback: If you know the IP, hardcode it here
    # ip = "10.x.x.x"   # ‚Üê replace with actual IP
    raise

URL = f"https://{ip}/api/v1/file/upload"

headers = {
    "Accept": "application/json",
    "Host": HOST
}

file_content = {
    "eventTime": datetime.now(timezone.utc).isoformat(),
    "policyId": "POL123",
    "status": "ACTIVE",
    "amount": 1000
}

files = {
    "file": ("test_data.json", json.dumps(file_content), "application/json")
}

try:
    print("üöÄ Sending upload...")
    response = requests.post(
        URL, headers=headers, files=files, 
        verify=False, timeout=30
    )
    print("Status Code:", response.status_code)
    print("Response:", response.text)
except Exception as e:
    print(f"‚ùå Error: {e}")
```

### What to do next

- Paste the output of the 3 diagnostic cells above.
- If **default resolution succeeds** ‚Üí use the socket code (no custom DNS needed).
- If **everything fails** ‚Üí the Spark pool has no network path to the internal DNS or the host.  
  **You will need to involve your network / Azure admin** to:
  - Add the DNS server to the VNet custom DNS settings.
  - Set up VNet peering or Private DNS Zone for `cta.allianz`.
  - Or get the current IP of `dhub-int.cta.allianz` from another machine (e.g., Azure VM in same network) and hardcode it temporarily.

Reply with the diagnostic results and I‚Äôll give the exact next fix.